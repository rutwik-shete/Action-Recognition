{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ECMM426/ECMM441 - Human Action Recognition.ipynb",
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<H1 style=\"text-align: center\">EEEM068 - Applied Machine Learning</H1>\n",
        "<H1 style=\"text-align: center\">Workshop 05</H1>\n",
        "<H1 style=\"text-align: center\">Human Action Recognition Tutorial</H1>\n",
        "\n"
      ],
      "metadata": {
        "id": "RakR_TVgNmE6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Introduction"
      ],
      "metadata": {
        "id": "vNvDHbU2zY1u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this tutorial, we will explore 2D and 3D convolutional neural network models in recognizing human actions occurring in the videos from the [KTH dataset](https://www.csc.kth.se/cvap/actions/)."
      ],
      "metadata": {
        "id": "1xVXuN1-zXI9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KTH Dataset\n",
        "The KTH dataset consists of videos of humans performing 6 types of action: *boxing*, *clapping*, *waving*, *jogging*, *running*, and *walking*. There are 25 subjects performing these actions in 4 scenarios: outdoor, outdoor with scale variation, outdoor with different clothes, and indoor. The total number of videos is therefore $25 \\times 4 \\times 6 = 600$. The videos' frame rate are 25fps and their resolution is $160 \\times 120$. More information about the dataset can be looked up at the website. More details on this dataset can be found at https://www.csc.kth.se/cvap/actions.\n",
        "\n",
        "We have preprocessed (i.e., extracted the frames and resized those frames etc) those videos and pickled them in the following link (https://empslocal.ex.ac.uk/people/staff/ad735/ECMM426/KTH_pickle.zip). In this experiment, we will be using the pickled version of the KTH dataset. However, the raw KTH videos can be found in this link (https://empslocal.ex.ac.uk/people/staff/ad735/ECMM426/KTH.zip).\n",
        "\n",
        "<img src=\"https://thumbs.gfycat.com/LivelyIdleArgali-max-1mb.gif\" alt=\"action recognition\" width=\"500\"/>"
      ],
      "metadata": {
        "id": "RBqUFnmzx21o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets download the pickled dataset from the above link."
      ],
      "metadata": {
        "id": "sVrBKULQUi5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset\n",
        "import os\n",
        "if not os.path.exists('KTH_pickle.zip'):\n",
        "  !wget --no-check-certificate https://empslocal.ex.ac.uk/people/staff/ad735/ECMM426/KTH_pickle.zip\n",
        "  !unzip -q KTH_pickle.zip\n",
        "\n",
        "# Dictionary of categories\n",
        "CATEGORY_INDEX = {\n",
        "    \"boxing\": 0,\n",
        "    \"handclapping\": 1,\n",
        "    \"handwaving\": 2,\n",
        "    \"jogging\": 3,\n",
        "    \"running\": 4,\n",
        "    \"walking\": 5\n",
        "}"
      ],
      "metadata": {
        "id": "JC1zNcUjwDmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example Video"
      ],
      "metadata": {
        "id": "9_3bTBMf26bP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets plot 20 equidistant frames of the very first video in the train split."
      ],
      "metadata": {
        "id": "8W-FdHna6gvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "videos = pickle.load(open('KTH_pickle/train.pickle', 'rb'))\n",
        "# first video\n",
        "video_0 = videos[0]['frames']\n",
        "# number of frames \n",
        "n = 20\n",
        "# figure size\n",
        "fig = plt.figure(figsize=(25, 25))\n",
        "# index of equidistant frames\n",
        "nth_frames = np.linspace(0, len(video_0) - 1, n).astype(int)\n",
        "for i in range(n):\n",
        "    frame = video_0[i]\n",
        "    fig.add_subplot(1, n, i + 1)\n",
        "    plt.imshow(frame, cmap='gray')\n",
        "    plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "print('Action class: ' + videos[0]['category'])"
      ],
      "metadata": {
        "id": "irg1Pter2_33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets plot 20 equidistant frames of the 91st video in the train split"
      ],
      "metadata": {
        "id": "_b_RgpUI-LEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tenth video\n",
        "video_90 = videos[90]['frames']\n",
        "# number of frames \n",
        "n = 20\n",
        "# figure size\n",
        "fig = plt.figure(figsize=(25, 25))\n",
        "# index of equidistant frames\n",
        "nth_frames = np.linspace(0, len(video_90) - 1, n).astype(int)\n",
        "for i in range(n):\n",
        "    frame = video_90[i]\n",
        "    fig.add_subplot(1, n, i + 1)\n",
        "    plt.imshow(frame, cmap='gray')\n",
        "    plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "print('Action class: ' + videos[90]['category'])"
      ],
      "metadata": {
        "id": "O22r78_C-Xpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset and DataLoader\n",
        "Since KTH dataset is not available within the torchvision's collection of datasets, we have to create our own dataset class to train an action recognition model. As discussed in the lecture, we will build two different models for that. The first one is based on 2D CNN, which will consider each frame as an image and will classify each frame into one of the action classes. Therefore, this model will work as an image classification model rather than a video classification model. For the second model, we will use 3D CNN which will consider a sequence or block of frames and intend to classify the entire sequence of frames into one of the action classes. In contrast with the first model, the second model will consider temporal information and will act as a true action recognition model.\n",
        "\n",
        "Therefore, in order to feed appropriate data, we will design two different types of dataset: (1) **SingleFrameDataset:** the first will return single frame which we will consider as an individual image, (2) **BlockFrameDataset:** block or sequence of frames where temporal information will be considered. Below, we have the two datasets for in PyTorch format."
      ],
      "metadata": {
        "id": "oOyC0Tolsu1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Single frame dataset"
      ],
      "metadata": {
        "id": "_w7xu7m-D6lZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset is used for training the single frame model."
      ],
      "metadata": {
        "id": "p30M6x3LD3L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import pickle\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class SingleFrameDataset(Dataset):\n",
        "    def __init__(self, directory, dataset=\"train\"):\n",
        "        self.instances, self.labels = self.read_dataset(directory, dataset)\n",
        "        # convert them into tensor\n",
        "        self.instances = torch.from_numpy(self.instances)\n",
        "        self.labels = torch.from_numpy(self.labels)\n",
        "        # normalize\n",
        "        self.zero_center()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.instances.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.instances[idx], self.labels[idx]\n",
        "\n",
        "    def zero_center(self):\n",
        "        self.instances -= float(self.mean)\n",
        "\n",
        "    def read_dataset(self, directory, dataset=\"train\"):\n",
        "        # set paths according to split\n",
        "        if dataset == \"train\":\n",
        "            filepath = os.path.join(directory, \"train.pickle\")\n",
        "        elif dataset == \"val\":\n",
        "            filepath = os.path.join(directory, \"val.pickle\")\n",
        "        else:\n",
        "            filepath = os.path.join(directory, \"test.pickle\")\n",
        "        #read the pickle file\n",
        "        videos = pickle.load(open(filepath, \"rb\"))\n",
        "        # accumulate the instances and label\n",
        "        instances = []\n",
        "        labels = []\n",
        "        for video in videos:\n",
        "            for frame in video[\"frames\"]:\n",
        "                instances.append(frame.reshape((1, 60, 80)))\n",
        "                labels.append(CATEGORY_INDEX[video[\"category\"]])\n",
        "        # numpy array\n",
        "        instances = np.array(instances, dtype=np.float32)\n",
        "        labels = np.array(labels, dtype=np.uint8)\n",
        "        self.mean = np.mean(instances)\n",
        "        return instances, labels"
      ],
      "metadata": {
        "id": "kJCZkUw0sxKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Block Frame Dataset"
      ],
      "metadata": {
        "id": "qKKppTFJEDMK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset is used for training the block frame model."
      ],
      "metadata": {
        "id": "oWUpk0tytWjH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import pickle\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class BlockFrameDataset(Dataset):\n",
        "    def __init__(self, directory, dataset=\"train\"):\n",
        "        self.instances, self.labels = self.read_dataset(directory, dataset)\n",
        "        # convert them into tensor\n",
        "        self.instances = torch.from_numpy(self.instances)\n",
        "        self.labels = torch.from_numpy(self.labels)\n",
        "        # normalize\n",
        "        self.zero_center()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.instances.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.instances[idx], self.labels[idx]\n",
        "\n",
        "    def zero_center(self):\n",
        "        self.instances -= float(self.mean)\n",
        "\n",
        "    def read_dataset(self, directory, dataset=\"train\", mean=None):\n",
        "        # set paths according to split\n",
        "        if dataset == \"train\":\n",
        "            filepath = os.path.join(directory, \"train.pickle\")\n",
        "        elif dataset == \"val\":\n",
        "            filepath = os.path.join(directory, \"val.pickle\")\n",
        "        else:\n",
        "            filepath = os.path.join(directory, \"test.pickle\")\n",
        "        # read the pickle file\n",
        "        videos = pickle.load(open(filepath, \"rb\"))\n",
        "        # accumulate the instances and label\n",
        "        instances = []\n",
        "        labels = []\n",
        "        current_block = []\n",
        "        for video in videos:\n",
        "            for i, frame in enumerate(video[\"frames\"]):\n",
        "                current_block.append(frame)\n",
        "                # 15 consecutive frames\n",
        "                if len(current_block) % 15 == 0:\n",
        "                    current_block = np.array(current_block)\n",
        "                    instances.append(current_block.reshape((1, 15, 60, 80)))\n",
        "                    current_block = []\n",
        "                    labels.append(CATEGORY_INDEX[video[\"category\"]])\n",
        "        # numpy array\n",
        "        instances = np.array(instances, dtype=np.float32)\n",
        "        labels = np.array(labels, dtype=np.uint8)\n",
        "        self.mean = np.mean(instances)\n",
        "        return instances, labels"
      ],
      "metadata": {
        "id": "J0uYc1o_tcFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models\n",
        "\n",
        "As mentioned above, we will create two different models. (1) **SingleFrameModel:** the first one is based on 2D CNN, which will consider each single frame as an image and will classify each frame into one of the action classes. In other words, this model will work as an image classification model rather than a video classification model, because it will not consider any temporal information. (2) **BlockFrameModel:** the second model we will use 3D CNN which will consider a sequence or block of frames and intend to classify the entire sequence of frames into one of the action classes. In contrast with the first model, the second model will consider temporal information and will act as a true action recognition model. Therefore, it is expected that the BlockFrameModel works better than the SingleFrameModel for the action classification task."
      ],
      "metadata": {
        "id": "YE5seYpPxY_f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Single Frame Model"
      ],
      "metadata": {
        "id": "eKlLD8NFFYoA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we implement the single frame model. If this model is tested after training it for 20 epochs, the accuracy of this model on the test set should be around 55%. This accuracy could be increased if you train it longer. In this model, we are going to use the following functions or modules:\n",
        "\n",
        "* `nn.Sequential()`: It is a sequential container. Modules will be added to it in the order they are passed in the constructor. Please check the [documentation](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential) for more details.\n",
        "\n",
        "* `nn.Conv2d()`: It is a PyTorch module that applies a 2D convolution over an input signal composed of several input planes. More details are available on the [documentation](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html).\n",
        "\n",
        "* `nn.BatchNorm2d()`: This module applies batch normalization over a 4D input as described in the [Batch Normalization paper](https://arxiv.org/abs/1502.03167). More details can be found in the [documentation](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html).\n",
        "\n",
        "* `nn.MaxPool2d()`: It is also a module that applies a 2D max pooling over an input signal composed of several input planes. Please have a look on this [documentation](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html) for more details.\n",
        "\n",
        "* `nn.Linear()`: It is a module that applies a linear transformation to the incoming data. More details can be found in its [documentation](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#linear).\n",
        "\n",
        "* `nn.ReLU()`: It is also a module that applies element-wise the rectified linear unit function. Its [documentation](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#relu) can explain more.\n",
        "\n",
        "* `nn.Dropout()`: This module randomly zeroes some of the elements of the input tensor with probability `p`. Check the [documentation](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#dropout) for more details."
      ],
      "metadata": {
        "id": "DtaTfXruFaZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SingleFrameModel(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(SingleFrameModel, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=5),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Conv2d(16, 32, kernel_size=3),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Dropout(0.5),       \n",
        "            nn.Conv2d(32, 64, kernel_size=3),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Dropout(0.5))\n",
        "        \n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(2560, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, n_classes))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "w_mka7mhwppH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Block Frame Model\n",
        "Below we implement the 2nd model that considers sequence of frames. For each video, we devide it into blocks of 15 contiguous frames. The model is then trained on these blocks instead of individual frame. In the convolutional layers, we use 3D convolutional filters (i.e. 3D CNN) to train the model to learn to detect temporal features.\n",
        "\n",
        "To classify a video, we also divide it into blocks of 15 contiguous frames. We then run the model on each block to get the block's vector of class probabilities. If this model is tested after training the model for 20 epochs the obtained accuracy on the test set should be around 67%. This means that the model is able to detect capture temporal information appeared in consecutive frames. However, this accuracy could be increased further by training it longer. In this model, we are going to use the following functions or modules:\n",
        "\n",
        "* `nn.Sequential()`: It is a sequential container. Modules will be added to it in the order they are passed in the constructor. Please check the [documentation](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential) for more details.\n",
        "\n",
        "* `nn.Conv3d()`: It is a PyTorch module that applies a 3D convolution over an input signal composed of several input planes. More details are available on the [documentation](https://pytorch.org/docs/stable/generated/torch.nn.Conv3d.html).\n",
        "\n",
        "* `nn.BatchNorm3d()`: This module applies batch normalization over a 5D input as described in the [Batch Normalization paper](https://arxiv.org/abs/1502.03167). More details can be found in the [documentation](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm3d.html).\n",
        "\n",
        "* `nn.MaxPool3d()`: It is also a module that applies a 3D max pooling over an input signal composed of several input planes. Please have a look on this [documentation](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool3d.html) for more details.\n",
        "\n",
        "* `nn.Linear()`: It is a module that applies a linear transformation to the incoming data. More details can be found in its [documentation](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#linear).\n",
        "\n",
        "* `nn.ReLU()`: It is also a module that applies element-wise the rectified linear unit function. Its [documentation](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#relu) can explain more.\n",
        "\n",
        "* `nn.Dropout()`: This module randomly zeroes some of the elements of the input tensor with probability `p`. Check the [documentation](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#dropout) for more details."
      ],
      "metadata": {
        "id": "5MjQIJjuxglx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class BlockFrameModel(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(BlockFrameModel, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv3d(1, 16, kernel_size=(4, 5, 5)),\n",
        "            nn.BatchNorm3d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool3d(kernel_size=(1, 2, 2)),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Conv3d(16, 32, kernel_size=(4, 3, 3)),\n",
        "            nn.BatchNorm3d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool3d(kernel_size=(2, 2, 2)),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Conv3d(32, 64, kernel_size=(3, 3, 3)),\n",
        "            nn.BatchNorm3d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool3d(kernel_size=(2, 2, 2)),\n",
        "            nn.Dropout(0.5))\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(2560, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, n_classes))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "2_FP6ZyGwLwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oap6-7RKG0s7"
      },
      "source": [
        "### Average Meter\n",
        "It is a simple class for keeping training statistics, such as losses and accuracies etc. The `.val` field usually holds the statistics for the current batch, whereas the `.avg` field hold statistics for the current epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeLH7fbOHDhH"
      },
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train and Test Functions\n",
        "Dataset/model independent train and test functions."
      ],
      "metadata": {
        "id": "F-zN4n7LJrAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import torch.nn.functional as F\n",
        "##define train function\n",
        "def train(model, data_loader, optimizer, device):\n",
        "    # meter\n",
        "    loss_meter = AverageMeter()\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "    tk = tqdm(data_loader, total=int(len(data_loader)), desc='Training', unit='frames', leave=False)\n",
        "    for batch_idx, data in enumerate(tk):\n",
        "        # fetch the data\n",
        "        frame, label = data[0], data[1]\n",
        "        # after fetching the data, transfer the model to the \n",
        "        # required device, in this example the device is gpu\n",
        "        # transfer to gpu can also be done by \n",
        "        frame, label = frame.to(device), label.to(device)\n",
        "        # compute the forward pass\n",
        "        output = model(frame)\n",
        "        # compute the loss function\n",
        "        loss_this = F.cross_entropy(output, label)\n",
        "        # initialize the optimizer\n",
        "        optimizer.zero_grad()\n",
        "        # compute the backward pass\n",
        "        loss_this.backward()\n",
        "        # update the parameters\n",
        "        optimizer.step()\n",
        "        # update the loss meter\n",
        "        loss_meter.update(loss_this.item(), label.shape[0])\n",
        "        tk.set_postfix({\"loss\": loss_meter.avg})\n",
        "    print('Train: Average loss: {:.4f}\\n'.format(loss_meter.avg))\n",
        "\n",
        "##define test function\n",
        "def test(model, data_loader, device):\n",
        "    # meters\n",
        "    loss_meter = AverageMeter()\n",
        "    acc_meter = AverageMeter()\n",
        "    # switch to test mode\n",
        "    correct = 0\n",
        "    model.eval()\n",
        "    tk = tqdm(data_loader, total=int(len(data_loader)), desc='Test', unit='frames', leave=False)\n",
        "    for batch_idx, data in enumerate(tk):\n",
        "        # fetch the data\n",
        "        frame, label = data[0], data[1]\n",
        "        # after fetching the data transfer the model to the \n",
        "        # required device, in this example the device is gpu\n",
        "        # transfer to gpu can also be done by \n",
        "        frame, label = frame.to(device), label.to(device)\n",
        "        # since we dont need to backpropagate loss in testing,\n",
        "        # we dont keep the gradient\n",
        "        with torch.no_grad():\n",
        "            output = model(frame)\n",
        "        # compute the loss function just for checking\n",
        "        loss_this = F.cross_entropy(output, label)\n",
        "        # get the index of the max log-probability\n",
        "        pred = output.argmax(dim=1, keepdim=True)\n",
        "        # check which of the predictions are correct\n",
        "        correct_this = pred.eq(label.view_as(pred)).sum().item()\n",
        "        # accumulate the correct ones\n",
        "        correct += correct_this\n",
        "        # compute accuracy\n",
        "        acc_this = correct_this / label.shape[0] * 100.0\n",
        "        # update the loss and accuracy meter \n",
        "        acc_meter.update(acc_this, label.shape[0])\n",
        "        loss_meter.update(loss_this.item(), label.shape[0])\n",
        "    print('Test: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        loss_meter.avg, correct, len(data_loader.dataset), acc_meter.avg))"
      ],
      "metadata": {
        "id": "SrWHdjf3waDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and Test"
      ],
      "metadata": {
        "id": "6s8RdNhKSzK1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Single Frame Model"
      ],
      "metadata": {
        "id": "mYQgGE9gS2PC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Parameters and Model Instantiation\n",
        "Select the correct dataset and model."
      ],
      "metadata": {
        "id": "sMWS0PreVZWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create Dataset\n",
        "from pathlib import Path\n",
        "dir_pickle = Path('KTH_pickle/')\n",
        "\n",
        "train_set = SingleFrameDataset(dir_pickle, \"train\")\n",
        "test_set = SingleFrameDataset(dir_pickle, \"test\")\n",
        "\n",
        "# 2. Create Dataloader\n",
        "from torch.utils.data import DataLoader\n",
        "batch_size = 64\n",
        "loader_args = dict(batch_size=batch_size, num_workers=1, pin_memory=True)\n",
        "train_loader = DataLoader(train_set, shuffle=True, **loader_args)\n",
        "test_loader = DataLoader(test_set, shuffle=False, drop_last=True, **loader_args)\n",
        "\n",
        "# 3. Create Model\n",
        "device = \"cuda\"\n",
        "model = SingleFrameModel(n_classes=6)\n",
        "model = model.to(device)\n",
        "\n",
        "# 4. Set up the optimizer, the loss, the learning rate scheduler and the loss scaling for AMP\n",
        "from torch import optim\n",
        "learning_rate = 0.001\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-8)"
      ],
      "metadata": {
        "id": "4mAUBPogU6aY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training Loop\n",
        "Training loop containing 20 training epochs. Test accuracies should be around 55%."
      ],
      "metadata": {
        "id": "-938nzOl1p4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epoch = 20\n",
        "for epoch in range(num_epoch):\n",
        "    train(model, train_loader, optimizer, device)\n",
        "test(model, test_loader, device)"
      ],
      "metadata": {
        "id": "ljL-MscZ1lou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Block Frame Model"
      ],
      "metadata": {
        "id": "Ggun7cQCTAQ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Parameters and Model Instantiation\n",
        "Select the correct dataset and model."
      ],
      "metadata": {
        "id": "XdS9XXjuTJuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create Dataset\n",
        "from pathlib import Path\n",
        "dir_pickle = Path('KTH_pickle/')\n",
        "\n",
        "train_set = BlockFrameDataset(dir_pickle, \"train\")\n",
        "test_set = BlockFrameDataset(dir_pickle, \"test\")\n",
        "\n",
        "# 2. Create Dataloader\n",
        "from torch.utils.data import DataLoader\n",
        "batch_size = 64\n",
        "loader_args = dict(batch_size=batch_size, num_workers=1, pin_memory=True)\n",
        "train_loader = DataLoader(train_set, shuffle=True, **loader_args)\n",
        "test_loader = DataLoader(test_set, shuffle=False, drop_last=True, **loader_args)\n",
        "\n",
        "# 3. Create Model\n",
        "device = \"cuda\"\n",
        "model = BlockFrameModel(n_classes=6)\n",
        "model = model.to(device)\n",
        "\n",
        "# 4. Set up the optimizer, the loss, the learning rate scheduler and the loss scaling for AMP\n",
        "from torch import optim\n",
        "learning_rate = 0.001\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-8)"
      ],
      "metadata": {
        "id": "bdAgghnaM4-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training Loop\n",
        "Training loop containing 20 training epochs. Test accuracies should be around 67%."
      ],
      "metadata": {
        "id": "TsPrD3fzYhG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epoch = 20\n",
        "for epoch in range(num_epoch):\n",
        "    train(model, train_loader, optimizer, device)\n",
        "test(model, test_loader, device)"
      ],
      "metadata": {
        "id": "qyf1Y_fZNDKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion\n",
        "If all goes well, the `BlockFrameModel()` should achieve superior performance than the `SingleFrameModel()` because the latter does not consider temporal information in a video which is crucial for video recognition."
      ],
      "metadata": {
        "id": "tTuOF4Fh2vpb"
      }
    }
  ]
}